{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Times New Roman;}}
{\colortbl ;\red255\green0\blue0;}
{\*\generator Riched20 10.0.22621}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\b\f0\fs32\lang9 LUNG CANCER DETECTION\b0\par
\par
\b\fs28 Step 1: Importing Libraries\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\par
\cf1 import tensorflow as tf\par
from tensorflow.keras import layers, models\par
import numpy as np\par
import matplotlib.pyplot as plt\par
from sklearn.model_selection import train_test_split\par
\cf0 ________________________________________________________________________\par
\b TensorFlow: \b0 A popular library for building and training machine learning models.\par
\b NumPy: \b0 A library for numerical operations in Python.\par
\b Matplotlib: \b0 A library for creating plots and visualizations.\par
\b scikit-learn (sklearn): \b0 A library for machine learning, which includes the train_test_split function used to split data into training and testing sets.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 2: Data Preparation (Dummy Data)\par
\b0\fs24 ________________________________________________________________________\par

\pard\sa200\sl276\slmult1\qj\cf1 # Assume X contains the image data and y contains the labels\par
# X should be a numpy array of shape (num_samples, height, width, channels)\par
# y should be a numpy array of shape (num_samples, 1)\par
\par
# For example, load data (replace this with actual data loading)\par
# X, y = load_your_lung_cancer_dataset()\par
\par
# For demonstration purposes, let's create dummy data\par
num_samples = 1000\par
height, width, channels = 128, 128, 1\par
X = np.random.rand(num_samples, height, width, channels)\par
y = np.random.randint(0, 2, (num_samples, 1))\par
\cf0 ________________________________________________________________________\par
\b X: \b0 The image data, represented as a NumPy array. Each image is 128x128 pixels with one color channel (grayscale).\par
\b y: \b0 The labels, indicating whether an image shows signs of lung cancer (1) or not (0).\par
\b Dummy data: \b0 Since actual data loading is not shown, we create dummy data. X contains random pixel values, and y contains random binary labels.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 3: Splitting Data\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Split the data into training and testing sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\par
\cf0 ________________________________________________________________________\par
train_test_split: Splits X and y into training and testing sets. 80% of the data is used for training (X_train, y_train), and 20% is used for testing (X_test, y_test).\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 4: Defining the CNN Model\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Define the CNN model\par
model = models.Sequential([\par
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width, channels)),\par
    layers.MaxPooling2D((2, 2)),\par
    layers.Conv2D(64, (3, 3), activation='relu'),\par
    layers.MaxPooling2D((2, 2)),\par
    layers.Conv2D(128, (3, 3), activation='relu'),\par
    layers.MaxPooling2D((2, 2)),\par
    layers.Flatten(),\par
    layers.Dense(128, activation='relu'),\par
    layers.Dense(1, activation='sigmoid')  # Binary classification\par
])\par
\cf0 ________________________________________________________________________\par
\b Sequential model: \b0 A linear stack of layers.\par
\b Conv2D layers: \b0 Convolutional layers that apply filters to the input image to extract features.\par
32, 64, 128: Number of filters in the layer.\par
(3, 3): The size of the filters.\par
\b activation='relu': \b0 The ReLU activation function introduces non-linearity.\par
\b input_shape=(height, width, channels): \b0 Specifies the shape of the input data.\par
\b MaxPooling2D layers: \b0 Reduce the spatial dimensions of the feature maps, helping to decrease the computational load and control overfitting.\par
\b (2, 2): \b0 The size of the pooling window.\par
\b Flatten layer: \b0 Flattens the 2D output of the convolutional layers into a 1D vector.\par
\b Dense layers: \b0 Fully connected layers.\par
\b 128: \b0 Number of neurons in the layer.\par
\b activation='relu': \b0 The ReLU activation function.\par
\b Output layer: \b0 A Dense layer with one neuron and a sigmoid activation function for binary classification.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 5: Compiling the Model\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Compile the model\par
model.compile(optimizer='adam',\par
              loss='binary_crossentropy',\par
              metrics=['accuracy'])\par
\cf0 ________________________________________________________________________\par
\b compile: \b0 Configures the model for training.\par
\b optimizer='adam': \b0 The Adam optimizer, a popular optimization algorithm.\par
\b loss='binary_crossentropy': \b0 The binary cross-entropy loss function, suitable for binary classification.\par
\b metrics=['accuracy']: \b0 Measures the accuracy of the model during training and evaluation.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 6: Training the Model\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Train the model\par
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\par
\cf0 ________________________________________________________________________\par
\b fit: \b0 Trains the model on the training data.\par
\b X_train, y_train: \b0 The training data and labels.\par
\b epochs=10: \b0 The number of times the model will pass through the entire training dataset.\par
\b batch_size=32: \b0 The number of samples per gradient update.\par
\b validation_split=0.2: \b0 20% of the training data is used for validation.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 7: Evaluating the Model\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Evaluate the model on the test set\par
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\par
print(f'\\nTest accuracy: \{test_acc\}')\par
\cf0 ________________________________________________________________________\par
\b evaluate: \b0 Evaluates the model on the test data.\par
\b X_test, y_test: \b0 The test data and labels.\par
\b verbose=2: \b0 Controls the verbosity of the output.\par
Prints the test accuracy.\par
\par

\pard\sa200\sl276\slmult1\qc\b\fs28 Step 8: Plotting Training History\b0\fs24\par

\pard\sa200\sl276\slmult1\qj ________________________________________________________________________\cf1 # Plot training & validation accuracy and loss values\par
plt.figure(figsize=(12, 4))\par
plt.subplot(1, 2, 1)\par
plt.plot(history.history['accuracy'], label='Train Accuracy')\par
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\par
plt.title('Model Accuracy')\par
plt.xlabel('Epoch')\par
plt.ylabel('Accuracy')\par
plt.legend()\par
\par
plt.subplot(1, 2, 2)\par
plt.plot(history.history['loss'], label='Train Loss')\par
plt.plot(history.history['val_loss'], label='Validation Loss')\par
plt.title('Model Loss')\par
plt.xlabel('Epoch')\par
plt.ylabel('Loss')\par
plt.legend()\par
\par
plt.show()\par
\cf0 ________________________________________________________________________\par
\b Plotting: \b0 Creates plots to visualize the training process.\par
\b Accuracy plot: \b0 Shows the training and validation accuracy over epochs.\par
\b Loss plot: \b0 Shows the training and validation loss over epochs.\par
\b history.history: \b0 Contains the accuracy and loss values recorded during training.\par
}
 